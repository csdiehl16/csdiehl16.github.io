<!DOCTYPE html><html lang="en" class="antialiased break-words"> <head><!-- Google tag (gtag.js) --><script type="text/partytown" async src="https://www.googletagmanager.com/gtag/js?id=G-NMTRKRDQZV"></script><script type="text/partytown">
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-NMTRKRDQZV');
        </script><!-- High Priority Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Building an AI-powered data explorer | Caleb Diehl</title><meta name="generator" content="Astro v4.16.18"><!-- Fonts --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400..700&family=Newsreader:ital,opsz,wght@0,6..72,400..700;1,6..72,400..700&display=swap" rel="stylesheet"><!-- Low Priority Global Metadata --><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="RSS"><!-- Page Metadata --><link rel="canonical" href="https://csdiehl.github.io/blog/post-3/"><meta name="description" content=""><!-- Open Graph / Facebook --><meta property="og:type" content="article"><meta property="og:url" content="https://csdiehl.github.io/blog/post-3/"><meta property="og:title" content="Building an AI-powered data explorer | Caleb Diehl"><meta property="og:description" content=""><meta property="og:image" content="https://csdiehl.github.io/ai_images/ai_scatter_example.png"><meta property="og:image:alt" content="An interactive notebook for visualizing data using AI"><!-- X/Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://csdiehl.github.io/blog/post-3/"><meta property="twitter:title" content="Building an AI-powered data explorer | Caleb Diehl"><meta property="twitter:description" content=""><meta property="twitter:image" content="https://csdiehl.github.io/ai_images/ai_scatter_example.png"><meta name="twitter:image:alt" content="An interactive notebook for visualizing data using AI"><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><link rel="stylesheet" href="/astro/carbon-calculator.CW9E0ZZC.css">
<style>h2{margin:2.5rem 8px}p,li{margin:1.5rem 8px;color:#181818;font-size:1rem;line-height:1.25rem}a{color:#37aecc;font-weight:700}code{font-family:Consolas,monospace;color:#d3d3d3}pre{padding:16px;border-radius:8px;margin:0 8px}
</style><script type="module" src="/astro/hoisted.YW3grJYV.js"></script>
<script>!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=Object.assign(w[p]||{},{"lib":"/~partytown/","debug":false});c[f]=(c[f]||[]).concat(["dataLayer.push"])})(window,'partytown','forward');/* Partytown 0.11.0 - MIT QwikDev */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,l,d,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(d=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(l=setTimeout(v,(null==s?void 0:s.fallbackTimeout)||1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(e){p=r.createElement(e?"script":"iframe"),t._pttab=Date.now(),e||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(e?"atomics.js?v=0.11.0":"sandbox-sw.html?"+t._pttab),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<d.length;n++)(o=r.createElement("script")).innerHTML=d[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(l)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);;(e=>{e.addEventListener("astro:before-swap",e=>{let r=document.body.querySelector("iframe[src*='/~partytown/']");if(r)e.newDocument.body.append(r)})})(document);</script></head> <body class="bg-main text-main"> <div class="flex flex-col min-h-screen"> <nav class="py-4 sticky top-0 bg-[#F2F1EB] flex items-start z-20" data-astro-cid-dmqpwcec> <div class="w-full max-w-3xl mx-6 relative" data-astro-cid-dmqpwcec> <button class="menu-toggle w-8 h-8 -ml-1 flex items-center justify-center relative z-30 md:hidden" aria-label="Open Menu" aria-expanded="false" aria-controls="menu-items" data-astro-cid-dmqpwcec> <span class="menu-toggle-icon w-6 h-px relative bg-current" data-astro-cid-dmqpwcec></span> </button> <ul id="menu-items" class="menu flex gap-6" data-astro-cid-dmqpwcec> <li class="py-1" data-astro-cid-dmqpwcec> <a class="text-xl hover:underline hover:underline-offset-2 hover:decoration-1 md:text-2xl md:font-semibold md:text-gray-600" href="/" data-astro-cid-dmqpwcec> Projects </a> </li><li class="py-1" data-astro-cid-dmqpwcec> <a class="text-xl hover:underline hover:underline-offset-2 hover:decoration-1 md:text-2xl md:font-semibold md:text-gray-600" href="/about" data-astro-cid-dmqpwcec> About </a> </li><li class="py-1" data-astro-cid-dmqpwcec> <a class="text-xl hover:underline hover:underline-offset-2 hover:decoration-1 md:text-2xl md:font-semibold md:text-gray-600" href="/blog" data-astro-cid-dmqpwcec> Blog </a> </li> </ul> </div> <div class="absolute right-0 top-4 z-10 md:top-8" data-astro-cid-dmqpwcec>  </div> </nav>   <main class="grow w-full mx-auto max-w-5xl">  <article class="relative z-0 flex flex-1 flex-col"> <div class="mx-auto w-full max-w-screen-lg md:-mb-10 lg:-mb-12 xl:-mb-16"> <img class="object-cover w-full min-h-[240px]" src="/ai_images/ai_scatter_example.png"> </div> <header class="p-8 bg-[#F2F1EC] max-w-screen-md mx-auto mb-8"> <h1 class="text-3xl leading-tight font-medium sm:text-5xl sm:leading-tight">Building an AI-powered data explorer</h1> <div class="mt-4 text-sm"> <time datetime="2024-03-29T07:00:00.000Z"> March 29, 2024 </time>  </div> </header> <div class="max-w-full md:max-w-screen-md text-md mx-auto"> <p>Exploring data should feel natural, like conducting an interview with an expert. However, analysts write their queries in code. It’s easy to get bogged down in syntax errors, forgetting parentheses, typing in complex chart specifications, or messing up variable names.</p>
<p>To reach that natural process of inquiry, I created an AI assistant for exploratory data analysis and prototyping visualizations. Here were my requirements:</p>
<ul>
<li>Ask natural language questions and get answers from a dataset.</li>
<li>Return simple visualizations and summary tables based on the questions, without me having to choose the visualizations or code them myself.</li>
</ul>
<p>I took advantage of <a href="https://sdk.vercel.ai/docs">Vercel’s AI SDK 3.0</a>, which uses <a href="https://nextjs.org/docs/app/building-your-application/rendering/server-components">React Server Components</a> to return streamable bits of UI from the AI back to the client. I combined this with <a href="https://observablehq.com/plot/">Observable Plot,</a> a high-level and flexible chart prototyping library to ask questions and get back charts.</p>
<div style="border:1px solid lightgrey;border-radius: 8px;padding:16px;">
Try out the deployed project <a style="font-weight:bold;" rel="noreferrer" target="_blank" href="https://ai-data-assistant.vercel.app/login">here! </a><br>
<a style="font-weight:bold;" rel="noreferrer" target="_blank" href="https://github.com/csdiehl/ai-data-assistant">Here's the repo</a> if you want to check out the code, run it locally, or contribute to the project!
</div>
<p><strong>In this post, I’ll walk through how I applied a few key AI concepts. Look out for these terms:</strong></p>
<ul>
<li>Function calling</li>
<li>Text-to-sql</li>
<li>Generative UI</li>
<li>Structured Output</li>
<li>Few-shot prompting</li>
</ul>
<p><img src="/ai_images/bitcoin_line.png" alt="An interactive notebook for visualizing data using AI"></p>
<h2 id="retrieving-data-from-natural-language-prompts">Retrieving data from natural language prompts</h2>
<p><strong>Essentially there are three steps in what I needed to do:</strong></p>
<ul>
<li>Turn a natural language query into a SQL, Javascript or Python expression</li>
<li>Run it and get back a Javascript array of objects</li>
<li>Plug that data into the ideal visualization, chosen through a combination of the LLM and hard-coded logical statements</li>
</ul>
<p>The first step was getting from a regular English question to a code snippet that can return data. Certain LLMs, including recent OpenAI models, can <strong>call functions.</strong> In the system prompt, you can reference functions that the LLM can use to augment its capabilities.</p>
<p>For example, LLMs are good at text, but bad at math. They only predict the next words in a sequence. So you could give it a “calculator” tool, and tell it to use that every time you need to add.</p>
<p>In my first crude version, I created different functions for different operations on the data. I uploaded JSON files and stored them in memory as Javascript arrays. I used methods from the <a href="https://d3js.org/">D3.js library</a> to sort, summarize and filter. I used <strong>few-shot prompting</strong> to hint at which tool to use, and what parameters to include.</p>
<p>This proved extremely tedious, because I had to write new functions for even simple operations, using Javascript, which is a terrible data analysis language. I needed to make queries specific to clue in the LLM to the right tool for the job.</p>
<p>I figured a much more flexible tool would be <strong><a href="https://python.langchain.com/docs/integrations/toolkits/sql_database">text-to-sql.</a></strong> This is an active research area in getting LLMs to take a natural language question and translate it into a syntactically correct SQL query.</p>
<p>Here’s what my data retrieval tool ended up looking like. The name and description fields help the LLM decided when it use the tool. The parameters, which I’ll detail later on, are arguments that the LLM will fill in and pass to a function to execute.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>  {</span></span>
<span class="line"><span>        name: "summarize_data",</span></span>
<span class="line"><span>        description:</span></span>
<span class="line"><span>          "Create a summary of the data, grouping one variable by another.",</span></span>
<span class="line"><span>        parameters:</span></span>
<span class="line"><span>        ...</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span></span></span></code></pre>
<p>In the system prompt, you can instruct the LLM to use it like this:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>To use your query to interact with the database, call \`summarize_data\`.</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<p>Taking this route meant that I needed a database.</p>
<h2 id="where-to-store-the-data">Where to store the data</h2>
<p>For this task, I didn’t need to persist the data beyond a single user session. I also just wanted to look at one dataset, or table, at a time. I wanted to stay focused on refining the LLM’s response to prompts and building a seamless <strong>generative UI.</strong></p>
<p>So in today’s ecosystem I had a lot of choices.</p>
<p><strong>I tried all of these things</strong></p>
<ul>
<li>Just store the data in React state, and use a library like <a href="https://github.com/AlaSQL/alasql">alasql</a> to make SQL-like queries to Javascript arrays</li>
<li>Store the data on the client using <a href="https://duckdb.org/docs/api/wasm/overview.html">DuckDB-WASM</a></li>
<li>Store the data on server in a persistent sqlite3 database file</li>
<li>Store the data in-memory on the server, using sqlite.</li>
</ul>
<p>I ended up going with option 4, with the intention of eventually shifting to option 3 so I can save a list of datasets I’m working on. Option 1 provided no opportunity to shift to more persistent storage later.</p>
<p>Option 2 was promising. DuckDB is a newer embedded database that’s supposed to be optimized for data analytics, compared to sqlite. DuckDB-WASM runs in the browser using web assembly. That means you can store all your data client-side and make SQL queries to it.</p>
<p>In fact, another (nice-looking and faster) version of what I did goes this route. You should <a href="https://www.duckbook.ai/">check this out</a> as well! I used that developer’s <a href="https://github.com/holdenmatt/duckdb-wasm-kit">open-source React bindings</a> for DuckDB-WASM to load a database in the browser.</p>
<p>But the catch was that I couldn’t access the client-side DB from the server, where all the AI action was happening. Although the chart component renders on the client side, the logic for rendering lives in the server component.</p>
<p>The closest solution I could think of was using <a href="https://legacy.reactjs.org/docs/context.html">React Context</a> to wrap the messages in a provider with the database, and access those from the Chart component. But this would make it difficult to eventually chain queries together - for example to get SQL, then turn that into natural language.</p>
<p>I ended up going with the in-memory <a href="https://www.npmjs.com/package/sqlite3">sqlite3</a> database set up on the server using Node sqlite. I used <a href="https://www.papaparse.com/">Papaparse</a> to parse CSVs, then some server-side code to extract the schema and other metadata and insert the rows into the db.</p>
<p><img src="/ai_images/ai_file_upload.png" alt="Uploading files to the AI data explorer"></p>
<p>The text-to-SQL pipeline greatly reduced the lines of code I needed. I only needed one flexible tool to provide to the LLM.</p>
<p>To ensure success, I included some critical guidance in the system prompt. The LLM is helpless without the database schema. It also helps to include a few sample rows. Think of what a human data analyst does - the first instinct is to check data types and column names, and peek at a few rows.</p>
<p>Vercel AI SDK allows you to track shared variables that are needed on both the client and server in something called the AI state. Normally, this is just an array of messages, but it can be any Javascript object. I modified it to include both a message array and certain metadata, like column names, about the file the user uploaded. The shared state looks like this:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span></span></span>
<span class="line"><span>// Define the initial state of the AI. It can be any JSON object.</span></span>
<span class="line"><span>const initialAIState: {</span></span>
<span class="line"><span>  sampleData: any[]</span></span>
<span class="line"><span>  dataKey: string</span></span>
<span class="line"><span>  columns: string[]</span></span>
<span class="line"><span>  tableName: string</span></span>
<span class="line"><span>  schema: string</span></span>
<span class="line"><span>  topK: number</span></span>
<span class="line"><span>  dataSummary: any[]</span></span>
<span class="line"><span>  messages: {</span></span>
<span class="line"><span>    role: "user" | "assistant" | "system" | "function"</span></span>
<span class="line"><span>    content: string</span></span>
<span class="line"><span>    id?: string</span></span>
<span class="line"><span>    name?: string</span></span>
<span class="line"><span>  }[]</span></span>
<span class="line"><span>} = {</span></span>
<span class="line"><span>  sampleData: [],</span></span>
<span class="line"><span>  dataKey: "",</span></span>
<span class="line"><span>  messages: [],</span></span>
<span class="line"><span>  columns: [],</span></span>
<span class="line"><span>  tableName: "",</span></span>
<span class="line"><span>  schema: "",</span></span>
<span class="line"><span>  topK: 5000,</span></span>
<span class="line"><span>  dataSummary: [], // this will hold all the data for a short time, then summaries</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<p>In my database setup function, I store the sample and schema in the shared UI and AI state, and inject these strings into the prompt.</p>
<h2 id="building-a-chart-from-a-data-summary">Building a chart from a data summary</h2>
<p>After asking a question, getting back SQL from the LLM and running this to retrieve data, I needed to visualize it.</p>
<p>I asked the LLM to generate some encodings (mappings of data to visual variables like color and size) at the same time it created the query. I used the <a href="https://zod.dev/">Zod</a> schema-mapping library to coerce its responses into a <strong>structured output.</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span></span></span>
<span class="line"><span>   x: z.string().describe("The x-axis variable."),</span></span>
<span class="line"><span>              y: z.string().optional().describe("The y-axis variable."),</span></span>
<span class="line"><span>              size: z</span></span>
<span class="line"><span>                .string()</span></span>
<span class="line"><span>                .optional()</span></span>
<span class="line"><span>                .describe("The variable to be represented by size."),</span></span>
<span class="line"><span>              color: z.optional(</span></span>
<span class="line"><span>                z.string().describe("The variable to be represented by color.")</span></span>
<span class="line"><span>              ),</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<p>This way, I could be sure I was getting all the required variables for the chart, in the right format, from the LLM’s choices. These included x, y, color, and size encoding, and a chart title. Similar to a <strong>few-shot prompting</strong> approach, I included guidance in the system prompt on when you might use certain chart types and encodings.</p>
<p>At first, I exerted more control - using Zod to force the encodings to actual variable names. This turned out to not be necessary. The LLM is great at taking messy input and figuring out which variable you’re talking about. You can ask about cars in the United States, and it will figure out you want to filter the Origin variable to USA. Often, the LLM makes up its own names as aliases in the SQL and uses those.</p>
<p>In a client-side component, I have some logic to take the LLM’s chart specification and render it using Observable Plot. The LLM still has trouble deciding on different charts, but generally does a good job, especially with additional prompting.</p>
<p>Of course, this structured approach limits the LLM’s creativity. Ideally, I wanted to let it generate its own chart code from scratch.</p>
<p>It did a great job generating code, but I as of right now, haven’t found a great way to convert the string that it returns into executable code. I tried using <a href="https://www.w3schools.com/jsref/jsref_eval.asp">Javascript’s eval()</a> and function constructor, which takes a string argument. However this is a dangerous, some say “evil” function, that can easily crash the application or create a security vulnerability.</p>
<h2 id="finishing-touches-with-streaming-ui">Finishing touches with streaming UI</h2>
<p>In many ways, it would have been easier to build this app using traditional client-server communication through REST APIs. I could have used <a href="https://js.langchain.com/docs/get_started/introduction">Langchain</a>, which is not as easy to integrate into Vercel’s Server Components setup as they make it sound. But I stuck with the server components mainly to test out a key new feature.</p>
<p>It’s called <a href="https://vercel.com/blog/how-streaming-helps-build-faster-web-applications">getStreamableUI,</a> and it’s awesome. Instead of streaming just chunks of text, you can stream entire React components. This allows you to build up complex UIs piece by piece, as the LLM returns information.</p>
<p>Here’s what it looks like. As soon as I receive the user message on the server, I immediately render a placeholder card to give instant feedback.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>  const reply = createStreamableUI(</span></span>
<span class="line"><span>    &#x3C;ResponseCard title={"thinking..."} caption={""}></span></span>
<span class="line"><span>      &#x3C;SkeletonChart /></span></span>
<span class="line"><span>    &#x3C;/ResponseCard></span></span>
<span class="line"><span>  )</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<p>Then I run the AI completion. As soon as the LLM comes back with a SQL query, we can update the UI with the query it’s running and the type of chart it plans to make.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span></span></span>
<span class="line"><span> completion.onFunctionCall("summarize_data", async ({ query, chartSpec }) => {</span></span>
<span class="line"><span>    const { x, y, title, type, color, size } = chartSpec</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    reply.update(</span></span>
<span class="line"><span>      &#x3C;ResponseCard title={title} caption={query}></span></span>
<span class="line"><span>        &#x3C;SkeletonChart></span></span>
<span class="line"><span>          Building {type} chart of {x}, {y}, {color}</span></span>
<span class="line"><span>        &#x3C;/SkeletonChart></span></span>
<span class="line"><span>      &#x3C;/ResponseCard></span></span>
<span class="line"><span>    )</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    // then start generating the chart</span></span>
<span class="line"><span></span></span>
<span class="line"><span> })</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<p>Finally, I actually run the query on the database and render the chart. When the UI is complete, we mark it as done.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span></span></span>
<span class="line"><span> const component =</span></span>
<span class="line"><span>      type === "table" ? (</span></span>
<span class="line"><span>        &#x3C;ResponseCard title={title} caption={query}></span></span>
<span class="line"><span>          &#x3C;Table data={response} xVar={x} /></span></span>
<span class="line"><span>        &#x3C;/ResponseCard></span></span>
<span class="line"><span>      ) : (</span></span>
<span class="line"><span>        &#x3C;ResponseCard title={title} caption={query}></span></span>
<span class="line"><span>          &#x3C;Chart</span></span>
<span class="line"><span>            type={type}</span></span>
<span class="line"><span>            data={response}</span></span>
<span class="line"><span>            dataKey={dataKey}</span></span>
<span class="line"><span>            x={x}</span></span>
<span class="line"><span>            y={y}</span></span>
<span class="line"><span>            size={size}</span></span>
<span class="line"><span>            color={color}</span></span>
<span class="line"><span>          /></span></span>
<span class="line"><span>        &#x3C;/ResponseCard></span></span>
<span class="line"><span>      )</span></span>
<span class="line"><span></span></span>
<span class="line"><span>    reply.done(component)</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span></code></pre>
<p>Finally, I added controls to make corrections when LLM goes wrong, such as flipping the chart, and showing the query. I aim to make data exporting and importing as seamless as possible. You can bring data from a csv or JSON at a URL, or on your computer, and export the LLM’s summaries as csv.</p>
<p>Here’s my attempt to make it produce the famous gapminder chart by Hans Rosling.</p>
<p><img src="/ai_images/ai_gapminder.png" alt="Gapminder chart generated by AI"></p>
<p>…Not quite…but we’re getting there.</p>
<h2 id="future-goals">Future goals</h2>
<p>This is just the beginning. One important feature I hope to implement next is follow-up questions. In the Vercel AI SDK you can keep track of message history in a shared state between AI and the client side, and feed this back into the model prompt on every call.</p>
<p>With the data analysis it’s a bit more complex because I need to direct the LLM to find and use the correct summarized dataset from the last relevant prompt.</p>
<p>In the future, it would be great to allow users to create accounts, save their analyses, and meter their usage of the OpenAI API. Until I implement these features, you can clone the project locally, swap in your own API key, and be up and running.</p>
<p>I’d love to collaborate with others on this. If you’d like to <a href="https://github.com/csdiehl/ai-data-assistant/issues">contribute to this project,</a> feel free to tackle an issue in the repo or open a PR for a new feature!</p> </div> <div class="mt-8 flex flex-wrap items-center gap-6 text-sm justify-between sm:mt-12 sm:text-base"> <div class="flex flex-wrap gap-x-5 gap-y-1 text-sm"> <a class="text-main hover:underline" href="/tags/javascript">
#Javascript </a><a class="text-main hover:underline" href="/tags/data-visualization">
#Data Visualization </a><a class="text-main hover:underline" href="/tags/ai">
#AI </a><a class="text-main hover:underline" href="/tags/llm">
#LLM </a><a class="text-main hover:underline" href="/tags/vercel-ai-sdk">
#Vercel AI SDK </a><a class="text-main hover:underline" href="/tags/open-ai">
#Open AI </a><a class="text-main hover:underline" href="/tags/observable">
#Observable </a> </div> <button class="inline-flex items-center justify-center px-6 py-2 font-serif text-sm leading-tight italic  text-main bg-main border border-main rounded-full transition hover:bg-muted copy-url-button" aria-label="Copy link" data-url="https://csdiehl.github.io/blog/post-3/" data-tooltip-default="Copy link" data-tooltip-success="Copied">Share</button> </div> </article> <div class="my-16 sm:my-24"> <h2 class="mb-12 text-xl font-serif italic sm:mb-16 sm:text-2xl">Read Next</h2>  <a class="flex justify-between items-start gap-8 group mb-10 sm:mb-12" href="/blog/post-2/"> <div class="grow"> <h3 class="text-xl leading-tight font-medium group-hover:underline group-hover:decoration-dashed group-hover:underline-offset-4 group-hover:decoration-1 sm:text-2xl">Observations on the new Observable 2.0 Framework</h3> <div class="mt-1 text-sm leading-normal"> <time datetime="2024-03-13T07:00:00.000Z"> March 13, 2024 </time>  </div> <div class="mt-3 text-sm leading-normal">My thoughts on Observable&#39;s latest creation.</div> </div> <div class="hidden font-serif italic opacity-0 transition group-hover:opacity-100 sm:inline-flex sm:gap-1 sm:items-center sm:shrink-0">
Read Post <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current w-4 h-4"> <path d="M4.286 12c0-0.533 0.432-0.964 0.964-0.964v0h11.172l-4.14-4.138c-0.175-0.175-0.283-0.416-0.283-0.683 0-0.533 0.432-0.965 0.965-0.965 0.267 0 0.508 0.108 0.683 0.283v0l5.785 5.785c0.175 0.175 0.283 0.416 0.283 0.683s-0.108 0.508-0.283 0.683l-5.785 5.785c-0.175 0.175-0.416 0.283-0.683 0.283-0.533 0-0.965-0.432-0.965-0.965 0-0.267 0.108-0.508 0.283-0.683v0l4.14-4.138h-11.172c-0.533 0-0.964-0.432-0.964-0.964v0z"></path> </svg> </div> </a> </div> </main> <footer class="w-full max-w-3xl mx-auto pt-12 pb-10 sm:pt-24 sm:pb-14"> <div class="mb-4 flex flex-wrap gap-x-6 gap-y-1"> <a class="font-serif hover:underline hover:underline-offset-2" href="/about"> About </a><a class="font-serif hover:underline hover:underline-offset-2" href="/contact"> Contact </a> </div> <div class="pt-6 flex flex-col gap-4 border-t border-dashed border-main sm:flex-row-reverse sm:justify-between sm:items-center"> <div class="flex flex-wrap gap-x-4 gap-y-1"> <a class="inline-flex items-center justify-center text-sm hover:underline hover:underline-offset-2" href="https://dribbble.com/" target="_blank" rel="noopener noreferer"> Linkedin </a><a class="inline-flex items-center justify-center text-sm hover:underline hover:underline-offset-2" href="https://instagram.com/" target="_blank" rel="noopener noreferer"> Github </a> </div> <p class="text-sm">
&copy; 2025&nbsp;<a class="hover:underline hover:underline-offset-2" href="/">Caleb Diehl</a>. All rights reserved.
</p> </div> </footer> </div> </body></html> 